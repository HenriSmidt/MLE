{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ab68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full(X.shape, 1 - p_drop))\n",
    "        X_drop = torch.where(mask == 1, torch.zeros_like(X), X) / (1 - p_drop)\n",
    "        return X_drop\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "def convolution_layer(previous_layer, weightvector, p_drop):\n",
    "    convolutional_layer = rectify(conv2d(previous_layer, weightvector))\n",
    "    \n",
    "    # reduces (2 ,2) window to 1 pixel\n",
    "    subsample_layer = max_pool2d(convolutional_layer, (2, 2))\n",
    "    out_layer = dropout(subsample_layer, p_drop_input)\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "def model(x, w_conv_1, w_conv_2, w_conv_3, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #print(x.shape) # [100, 1, 28, 28]\n",
    "    h_conv_1 = convolution_layer(x, w_conv_1, p_drop_input)\n",
    "    #print(h_conv_1.shape) # [100, 32, 12, 12]\n",
    "    h_conv_2 = convolution_layer(h_conv_1, w_conv_2, p_drop_hidden)\n",
    "    #print(h_conv_2.shape) # [100, 64, 4, 4]\n",
    "    h_conv_3 = convolution_layer(h_conv_2, w_conv_3, p_drop_hidden)\n",
    "    #print(h_conv_3.shape)  # [100, 128, 1, 1]\n",
    "    h_conv_3 = h_conv_3.reshape(100, -1)\n",
    "    #print(h_conv_3.shape)  # [100, 128]\n",
    "    h2 = rectify(h_conv_3 @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "def test_model(x, w_conv_1, w_conv_2, w_conv_3, w_h2, w_o):\n",
    "    #dropout of -1 means no dropout\n",
    "    h_conv_1 = convolution_layer(x, w_conv_1, -1)\n",
    "    h_conv_2 = convolution_layer(h_conv_1, w_conv_2, -1)\n",
    "    h_conv_3 = convolution_layer(h_conv_2, w_conv_3, -1)\n",
    "    h_conv_3 = h_conv_3.reshape(100, -1)\n",
    "\n",
    "    h2 = rectify(h_conv_3 @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "w_conv_1 = init_weights((32, 1, 5, 5))\n",
    "w_conv_2 = init_weights((64, 32, 5, 5))\n",
    "w_conv_3 = init_weights((128, 64, 2, 2))\n",
    "\n",
    "number_of_output_pixel = 128\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((number_of_output_pixel, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_conv_1, w_conv_2, w_conv_3, w_h2, w_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "p_drop_input = 0.4\n",
    "p_drop_hidden = 0.4\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "        #print(x.shape)\n",
    "        # our model requires flattened input\n",
    "        #x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = model(x, w_conv_1, w_conv_2, w_conv_3, w_h2, w_o, p_drop_input, p_drop_hidden)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                #x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = test_model(x, w_conv_1, w_conv_2, w_conv_3, w_h2, w_o)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9db3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
