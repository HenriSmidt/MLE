{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "#set device for potential GPU training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Cuda available: {}\".format(torch.cuda.is_available()))\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    # move variable to GPU\n",
    "    w = w.to(device)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o):\n",
    "    h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # Move data to the GPU device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "\n",
    "                # Move data to the GPU device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "# Move the model parameters back to the CPU device\n",
    "w_h = w_h.cpu()\n",
    "w_h2 = w_h2.cpu()\n",
    "w_o = w_o.cpu()\n",
    "\n",
    "train_loss = [loss.item() for loss in train_loss]\n",
    "test_loss = [loss.item() for loss in test_loss]\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        mask = torch.bernoulli(torch.full(X.shape, 1 - p_drop))\n",
    "        X_drop = torch.where(mask == 1, torch.zeros_like(X), X) / (1 - p_drop)\n",
    "        return X_drop\n",
    "    else:\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_model(x, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    x_drop = dropout(x, p_drop_input)\n",
    "    h = rectify(x_drop @ w_h)\n",
    "    h_drop = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h_drop @ w_h2)\n",
    "    h2_drop = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2_drop @ w_o\n",
    "    \n",
    "    return pre_softmax\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Explain in a few sentences how the dropout method works and how it reduces overfitting.\n",
    "\n",
    "Answer: During training, dropout randomly sets a fraction of input units or hidden units to zero at each update, effectively \"dropping out\" those units. This prevents the model from relying to heavily on specific input or hidden units, as they are sometimes \"dropped out\". During testing you obviously dont apply dropout. Instead you scale the input units units with the factor of 1-p_drop to compensate for the missing dropout.\n",
    "\n",
    "Task: Why do we need a different model configuration for evaluating the test loss? \n",
    "\n",
    "Answer: During testing you obviously dont apply dropout, as this only leaves out information and simply wouldnt make any sense. Instead you scale the input units units with the factor of 1-p_drop to compensate for the missing dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(x, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "\n",
    "    scaled_x = (1 - p_drop_input) * x\n",
    "    h = rectify(scaled_x @ w_h)\n",
    "    scaled_h = (1 - p_drop_hidden) * h\n",
    "    h2 = rectify(scaled_h @ w_h2)\n",
    "    scaled_h2 = (1 - p_drop_hidden) * h2\n",
    "    pre_softmax = scaled_h2 @ w_o\n",
    "    \n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 1.06e+00\n",
      "Mean Test Loss:  4.95e-01\n",
      "Epoch: 10\n",
      "Mean Train Loss: 8.40e-01\n",
      "Mean Test Loss:  3.39e-01\n",
      "Epoch: 20\n",
      "Mean Train Loss: 1.02e+00\n",
      "Mean Test Loss:  2.75e-01\n",
      "Epoch: 30\n",
      "Mean Train Loss: 1.10e+00\n",
      "Mean Test Loss:  2.56e-01\n",
      "Epoch: 40\n",
      "Mean Train Loss: 1.19e+00\n",
      "Mean Test Loss:  2.74e-01\n",
      "Epoch: 50\n",
      "Mean Train Loss: 1.25e+00\n",
      "Mean Test Loss:  2.54e-01\n",
      "Epoch: 60\n",
      "Mean Train Loss: 1.33e+00\n",
      "Mean Test Loss:  2.94e-01\n",
      "Epoch: 70\n",
      "Mean Train Loss: 1.35e+00\n",
      "Mean Test Loss:  2.77e-01\n",
      "Epoch: 80\n",
      "Mean Train Loss: 1.41e+00\n",
      "Mean Test Loss:  2.85e-01\n",
      "Epoch: 90\n",
      "Mean Train Loss: 1.45e+00\n",
      "Mean Test Loss:  3.14e-01\n",
      "Epoch: 100\n",
      "Mean Train Loss: 1.47e+00\n",
      "Mean Test Loss:  2.93e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14ca248cb50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=False,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=False,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = dropout_model(x, w_h, w_h2, w_o, 0.4, 0.4)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = test_model(x, w_h, w_h2, w_o, 0.4, 0.4)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Compare the test error with the test error from Section 1.\n",
    "\n",
    "Answer: \n",
    "\n",
    "The training error in Section 1 is continously decreasing, even during the last epochs. However, the test error in Section 1 even increases slightly over time. This is a clear indication that the model overfits on the training data.\n",
    "\n",
    "The training error in the Section with dropout doesnt really converge. The test error decreases slightly over time and is overall better than in the run without the dropout. This is a clear indication, that the model is better than the one from section 1 and does not overfit on the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Parametric Relu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to do some edits to allow for training and testing with a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "#set device for potential GPU training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Cuda available: {}\".format(torch.cuda.is_available()))\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    # move variable to GPU\n",
    "    w = w.to(device)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create a PRelu layer by mapping $X \\rightarrow  PRelu(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "    return torch.max(torch.zeros_like(X), X) + a * torch.min(torch.zeros_like(X), X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further we are going to incorporate the parameter a into the params list and optimize them during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 4.34e-01\n",
      "Mean Test Loss:  1.65e-01\n",
      "Epoch: 10\n",
      "Mean Train Loss: 1.10e-01\n",
      "Mean Test Loss:  2.60e-01\n",
      "Epoch: 20\n",
      "Mean Train Loss: 7.73e-02\n",
      "Mean Test Loss:  2.02e-01\n",
      "Epoch: 30\n",
      "Mean Train Loss: 6.30e-02\n",
      "Mean Test Loss:  3.26e-01\n",
      "Epoch: 40\n",
      "Mean Train Loss: 4.77e-02\n",
      "Mean Test Loss:  3.47e-01\n",
      "Epoch: 50\n",
      "Mean Train Loss: 3.88e-02\n",
      "Mean Test Loss:  3.80e-01\n",
      "Epoch: 60\n",
      "Mean Train Loss: 3.96e-02\n",
      "Mean Test Loss:  4.28e-01\n",
      "Epoch: 70\n",
      "Mean Train Loss: 3.94e-02\n",
      "Mean Test Loss:  4.28e-01\n",
      "Epoch: 80\n",
      "Mean Train Loss: 2.82e-02\n",
      "Mean Test Loss:  4.65e-01\n",
      "Epoch: 90\n",
      "Mean Train Loss: 3.22e-02\n",
      "Mean Test Loss:  4.84e-01\n",
      "Epoch: 100\n",
      "Mean Train Loss: 2.92e-02\n",
      "Mean Test Loss:  4.69e-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x281288c4350>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o, a):\n",
    "    h = PRelu(x @ w_h, a)\n",
    "    h2 = PRelu(h @ w_h2, a)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax\n",
    "\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# input shape is (B, 784)\n",
    "w_h = init_weights((784, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_h2 = init_weights((625, 625))\n",
    "# hidden layer with 625 neurons\n",
    "w_o = init_weights((625, 10))\n",
    "# output shape is (B, 10)\n",
    "\n",
    "\n",
    "# initialize PRelu parameter a with 0.25 according to paper\n",
    "# http://arxiv.org/pdf/1502.01852.pdf\n",
    "a = torch.tensor([0.25], requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = RMSprop(params=[w_h, w_h2, w_o, a])\n",
    "#optimizer = optimizer.to(device)\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train_loss_this_epoch = []\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        x, y = batch\n",
    "\n",
    "        # Move data to the GPU device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # our model requires flattened input\n",
    "        x = x.reshape(batch_size, 784)\n",
    "        # feed input through model\n",
    "        noise_py_x = model(x, w_h, w_h2, w_o, a)\n",
    "\n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the cross-entropy loss function already contains the softmax\n",
    "        loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "        train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "    # test periodically\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "        test_loss_this_epoch = []\n",
    "\n",
    "        # no need to compute gradients for validation\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_dataloader):\n",
    "                x, y = batch\n",
    "\n",
    "                # Move data to the GPU device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                x = x.reshape(batch_size, 784)\n",
    "                noise_py_x = model(x, w_h, w_h2, w_o, a)\n",
    "\n",
    "                loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "        test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "        print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "\n",
    "# Move the model parameters back to the CPU device\n",
    "w_h = w_h.cpu()\n",
    "w_h2 = w_h2.cpu()\n",
    "w_o = w_o.cpu()\n",
    "a = a.cpu()\n",
    "\n",
    "train_loss = [loss.item() for loss in train_loss]\n",
    "test_loss = [loss.item() for loss in test_loss]\n",
    "\n",
    "plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "plt.title(\"Train and Test Loss over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
